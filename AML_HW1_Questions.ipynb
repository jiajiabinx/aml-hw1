{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6edde57",
      "metadata": {
        "id": "c6edde57"
      },
      "source": [
        "## Homework 1: Applied Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f40f8a9",
      "metadata": {
        "id": "1f40f8a9"
      },
      "source": [
        "This assignment covers contents of the first three lectures.\n",
        "\n",
        "The emphasis for this assignment would be on the following:\n",
        "1. Data Visualization and Analysis\n",
        "2. Linear Models for Regression and Classification\n",
        "3. Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642c4680",
      "metadata": {
        "id": "642c4680"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e68e5c",
      "metadata": {
        "id": "84e68e5c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import inv\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.svm import LinearSVC, SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1774cb2b",
      "metadata": {
        "id": "1774cb2b"
      },
      "source": [
        "## **Part 1: Data Visualization and Analysis**\n",
        "\n",
        "Understanding data characteristics and patterns is crucial for building effective models. In this part, we will visualize and analyze the `insurance.csv` dataset.\n",
        "\n",
        "<b>Note: Remember to label plot axes while plotting.</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c50ed1",
      "metadata": {
        "id": "08c50ed1"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "insurance_df = pd.read_csv('insurance.csv')\n",
        "insurance_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87206a42",
      "metadata": {
        "id": "87206a42"
      },
      "source": [
        "**1.1 Create a bar chart to compare the average insurance charges by sex and region.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7461ed0a",
      "metadata": {
        "id": "7461ed0a"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8dbf25",
      "metadata": {
        "id": "ee8dbf25"
      },
      "outputs": [],
      "source": [
        "### Comment here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a99ddb",
      "metadata": {
        "id": "c0a99ddb"
      },
      "source": [
        "**1.2 Plot a small multiple of bar charts to visualize the data distribution for the following categorical variables:**\n",
        "1. **sex**\n",
        "2. **region**\n",
        "3. **children**\n",
        "4. **smoker**\n",
        "\n",
        "**Make subplots in the same graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7d105f",
      "metadata": {
        "id": "4a7d105f",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697540c5",
      "metadata": {
        "id": "697540c5"
      },
      "source": [
        "**1.3 Compare the insurance charges by age and smoker. Create a Scatter plot for age vs insurance charges categorize them by smoker type.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa9ee9e",
      "metadata": {
        "id": "daa9ee9e"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5f4bea",
      "metadata": {
        "id": "3c5f4bea"
      },
      "source": [
        "## **Part 2: Linear Models for Regression and Classification**\n",
        "\n",
        "In this section, we will be implementing three linear models **linear regression, logistic regression, and SVM**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7987c6bc",
      "metadata": {
        "id": "7987c6bc"
      },
      "source": [
        "### **2.1 Linear Regression**\n",
        "We will now proceed with splitting the dataset and implementing linear regression to predict `insurance charges`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c434d784",
      "metadata": {
        "id": "c434d784"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into features and labels\n",
        "insurance_X = insurance_df.drop(columns=['charges'])\n",
        "insurance_y = insurance_df['charges']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bea9b3",
      "metadata": {
        "id": "e1bea9b3"
      },
      "source": [
        "**2.1.1 Plot relationships between features (age, bmi, children, region) and the target variable `charges` as a small multiple of scatter plots.**\n",
        "1. age\n",
        "2. bmi\n",
        "3. children\n",
        "4. region\n",
        "\n",
        "Make sure to label the axes.\n",
        "<b></b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b82794",
      "metadata": {
        "id": "e1b82794"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1618ce",
      "metadata": {
        "id": "7b1618ce"
      },
      "source": [
        "**2.1.2 From the visualizations above, do you think linear regression is a good model for this problem? Why and/or why not? Please explain.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3124acb5",
      "metadata": {
        "id": "3124acb5"
      },
      "outputs": [],
      "source": [
        "### Comment here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9294a46b",
      "metadata": {
        "id": "9294a46b"
      },
      "source": [
        "### **Data Preprocessing**\n",
        "\n",
        "Before we can fit a linear regression model, several pre-processing steps should be applied to the dataset:\n",
        "\n",
        "1. **Encode categorical features appropriately** (e.g., `sex`, `smoker`, `region`).\n",
        "2. **Check for multicollinearity** by analyzing the correlation matrix and removing any highly collinear features.\n",
        "3. **Split the dataset** into training (60%), validation (20%), and test (20%) sets.\n",
        "4. **Standardize the feature matrices** (`X_train`, `X_val`, and `X_test`) to have zero mean and unit variance. Ensure that the standardization parameters (mean, variance) are learned from `X_train` and then applied to all sets to avoid information leakage.\n",
        "5. **Add a column of ones** to `X_train`, `X_val`, and `X_test` for learning the bias term in the linear model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2ac0718",
      "metadata": {
        "id": "d2ac0718"
      },
      "source": [
        "**2.1.3 Encode the categorical variables of the Insurance dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb54f38",
      "metadata": {
        "id": "2bb54f38"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccaf805",
      "metadata": {
        "id": "3ccaf805"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4480b678",
      "metadata": {
        "id": "4480b678"
      },
      "source": [
        "**2.1.4 Plot the correlation matrix, and check if there is high correlation between the given numerical features (Threshold >= 0.8). If yes, drop one from each pair of highly correlated features from the dataframe. It is fine if you do not find any highly correlated features. Why could this be necessary before proceeding further?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "310a012f",
      "metadata": {
        "id": "310a012f"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c93816",
      "metadata": {
        "id": "c7c93816"
      },
      "outputs": [],
      "source": [
        "### Comment here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1c5a5e3",
      "metadata": {
        "id": "f1c5a5e3"
      },
      "source": [
        "**2.1.5 Split the dataset into training (60%), validation (20%), and test (20%) sets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80578dbe",
      "metadata": {
        "id": "80578dbe"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791eb562",
      "metadata": {
        "id": "791eb562"
      },
      "source": [
        "**2.1.6 Standardize the columns in the feature matrices.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b8cfc0",
      "metadata": {
        "id": "17b8cfc0"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eda1947",
      "metadata": {
        "id": "4eda1947"
      },
      "outputs": [],
      "source": [
        "# Adding a column of ones to include the bias term\n",
        "\n",
        "# insurance_X_train = np.hstack([np.ones((insurance_X_train.shape[0], 1)), insurance_X_train])\n",
        "# insurance_X_val = np.hstack([np.ones((insurance_X_val.shape[0], 1)), insurance_X_val])\n",
        "# insurance_X_test = np.hstack([np.ones((insurance_X_test.shape[0], 1)), insurance_X_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65dcad23",
      "metadata": {
        "id": "65dcad23"
      },
      "source": [
        "At the end of this pre-processing, you should have the following vectors and matrices:\n",
        "\n",
        "- **insurance_X_train**: Training set feature matrix.\n",
        "- **insurance_X_val**: Validation set feature matrix.\n",
        "- **insurance_X_test**: Test set feature matrix.\n",
        "- **insurance_y_train**: Training set labels (insurance charges).\n",
        "- **insurance_y_val**: Validation set labels.\n",
        "- **insurance_y_test**: Test set labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b79711b",
      "metadata": {
        "id": "9b79711b"
      },
      "source": [
        "### Implement Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a984648",
      "metadata": {
        "id": "6a984648"
      },
      "source": [
        "Now that the data is preprocessed, we can implement a linear regression model, specifically Ridge Regression, which incorporates L2 regularization.\n",
        "\n",
        "Given a feature matrix \\( X \\), a label vector \\( y \\), and a weight vector \\( w \\), the hypothesis function for linear regression is:\n",
        "\n",
        "$$\n",
        "y = X w\n",
        "$$\n",
        "\n",
        "The objective is to find the optimal weight vector \\( w \\) that minimizes the following loss function:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\min_{w} \\| X w - y \\|^2_2 + \\alpha \\| w \\|^2_2 \\\\\n",
        "$$\n",
        "\n",
        "Where:\n",
        "-  $\\| X w - y \\|^2_2$ penalizes predictions that differ from actual labels.\n",
        "- $\\alpha \\| w \\|^2_2$ is the regularization term, helping reduce overfitting by penalizing large weights.\n",
        "-  $\\alpha$ is the regularization parameter.\n",
        "\n",
        "The closed-form solution for Ridge Regression is given by the Normal Equations:\n",
        "\n",
        "$$\n",
        "w = (X^T X + \\alpha I)^{-1} X^T y\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e4de9f1",
      "metadata": {
        "id": "6e4de9f1"
      },
      "source": [
        "**2.1.7 Implement a `LinearRegression` class with `train` and `predict` methods**\n",
        "\n",
        "We will now implement a custom `LinearRegression` class with L2 regularization (Ridge Regression).\n",
        "\n",
        "**Note: You may NOT use sklearn for this implementation. You may, however, use `np.linalg.solve` to find the closed-form solution. It is highly recommended that you vectorize your code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ed787ee",
      "metadata": {
        "id": "4ed787ee"
      },
      "outputs": [],
      "source": [
        "class LinearRegression():\n",
        "    '''\n",
        "    Linear regression model with L2-regularization (i.e. ridge regression).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    alpha: regularization parameter\n",
        "    w: (n x 1) weight vector\n",
        "    '''\n",
        "    def __init__(self, alpha=0):\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        '''Trains model using ridge regression closed-form solution.\n",
        "        Parameters:\n",
        "        X : (m x n) feature matrix\n",
        "        y: (m x 1) label vector\n",
        "        '''\n",
        "        ### Your code here\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''Predicts on X using trained model.\n",
        "        Parameters:\n",
        "        X : (m x n) feature matrix\n",
        "        Returns:\n",
        "        y_pred: (m x 1) prediction vector\n",
        "        '''\n",
        "        ### Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd639fe",
      "metadata": {
        "id": "8cd639fe"
      },
      "source": [
        "**2.1.8 Train, Evaluate, and Interpret Linear Regression Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438464ca",
      "metadata": {
        "id": "438464ca"
      },
      "source": [
        "**Train a linear regression model ($\\alpha = 0$) on the insurance dataset. Make predictions and report the $R^2$ score on the training, validation, and test sets. Report the first 3 and last 3 predictions on the test set, along with the actual labels.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4d43b6",
      "metadata": {
        "id": "1c4d43b6"
      },
      "outputs": [],
      "source": [
        "def get_report(y_pred, y_test):\n",
        "    \"\"\"\n",
        "    Report the first 3 and last 3 predictions on X_test,\n",
        "    along with the actual labels in y_test.\n",
        "\n",
        "    Returns:\n",
        "        A dataframe with 6 rows comparing predictions and actuals.\n",
        "    \"\"\"\n",
        "    preds = np.concatenate([y_pred[:3], y_pred[-3:]])\n",
        "    actuals = np.concatenate([y_test[:3], y_test[-3:]])\n",
        "    df_compare = pd.DataFrame({'Prediction': preds,\n",
        "                               'Actual': actuals})\n",
        "    df_compare['Position'] = [1, 2, 3, len(y_pred) - 2, len(y_pred) - 1, len(y_pred)]\n",
        "    df_compare = df_compare.set_index('Position')\n",
        "    return df_compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684f8311",
      "metadata": {
        "id": "684f8311"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b021423",
      "metadata": {
        "id": "4b021423"
      },
      "source": [
        "**2.1.9 Use the mean of the training labels (insurance_y_train) as the prediction for all instances. Report the $R^2$ on the training, validation, and test sets using this baseline.**\n",
        "\n",
        ">This is a common baseline used in regression problems and tells you if your model is any good. Your linear regression $R^2$ should be much higher than these baseline $R^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871a3cd4",
      "metadata": {
        "id": "871a3cd4"
      },
      "outputs": [],
      "source": [
        "### Code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a67beb0",
      "metadata": {
        "id": "7a67beb0"
      },
      "source": [
        "**2.1.10 Interpret your model trained on the insurance dataset using a bar chart of the model weights. Make sure to label the bars (x-axis) and don't forget the bias term!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd80387c",
      "metadata": {
        "id": "cd80387c"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd42c263",
      "metadata": {
        "id": "cd42c263"
      },
      "source": [
        "**2.1.11 According to your model, which features are the greatest contributors to insurance charges?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95a7a07",
      "metadata": {
        "id": "c95a7a07"
      },
      "outputs": [],
      "source": [
        "### Comment here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30bb5238",
      "metadata": {
        "id": "30bb5238"
      },
      "source": [
        "### **Hyperparameter Tuning ($\\alpha$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929606d4",
      "metadata": {
        "id": "929606d4"
      },
      "source": [
        "Now, let's tune the $\\alpha$ regularization parameter for ridge regression on the insurance dataset.\n",
        "\n",
        "**2.1.12 Sweep out values for $\\alpha$ using `alphas = np.logspace(-5, 1, 20)`. Perform a grid search over these $\\alpha$ values, recording the training and validation $R^2$ for each $\\alpha$. Plot the results with a log scale for $\\alpha$. A simple grid search is fine, no need for k-fold cross validation. Plot the training and validation $R^2$ as a function of $\\alpha$ on a single figure. Make sure to label the axes and the training and validation $R^2$ curves. Use a log scale for the x-axis.****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f489baeb",
      "metadata": {
        "id": "f489baeb"
      },
      "outputs": [],
      "source": [
        "### Code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85a1019",
      "metadata": {
        "id": "b85a1019"
      },
      "source": [
        "**2.1.13 Explain your plot above. How do training and validation $R^2$ behave with increasing $\\alpha$?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c65f2ff",
      "metadata": {
        "id": "3c65f2ff"
      },
      "outputs": [],
      "source": [
        "### Comment here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203ddf55",
      "metadata": {
        "id": "203ddf55"
      },
      "source": [
        "### 2.2 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a60b21",
      "metadata": {
        "id": "58a60b21"
      },
      "source": [
        "**2.2.1 Load the dataset, the dataset to be used is loan_data.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983a00c2",
      "metadata": {
        "id": "983a00c2"
      },
      "outputs": [],
      "source": [
        "### Code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d643e940",
      "metadata": {
        "id": "d643e940"
      },
      "outputs": [],
      "source": [
        "# loan_data_df = loan_data_df.drop(columns=['Loan_ID'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a78ac06",
      "metadata": {
        "id": "4a78ac06"
      },
      "source": [
        "**2.2.2 Are there any missing values in the dataset? If so, what is the best way to deal with it and why?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd2a5ff",
      "metadata": {
        "id": "6bd2a5ff"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc64baf",
      "metadata": {
        "id": "9cc64baf"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da916a4f",
      "metadata": {
        "id": "da916a4f"
      },
      "outputs": [],
      "source": [
        "### Comment here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6acd74cb",
      "metadata": {
        "id": "6acd74cb"
      },
      "source": [
        "**2.2.3 Encode the categorical variables.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7936a96",
      "metadata": {
        "id": "c7936a96"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d55bc6d2",
      "metadata": {
        "id": "d55bc6d2"
      },
      "source": [
        "**2.2.4 Do you think that the distribution of labels is balanced? Why/why not? Hint: Find the probability of the different categories.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9af3de4",
      "metadata": {
        "id": "c9af3de4"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707a75a6",
      "metadata": {
        "id": "707a75a6"
      },
      "outputs": [],
      "source": [
        "### Comment here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d8dbc8",
      "metadata": {
        "id": "34d8dbc8"
      },
      "source": [
        "**2.2.5 Plot the correlation matrix (first separate features and Y variable), and check if there is high correlation between the given numerical features (Threshold >=0.9). If yes, drop those highly correlated features from the dataframe.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99ba5893",
      "metadata": {
        "id": "99ba5893"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78c4fef0",
      "metadata": {
        "id": "78c4fef0"
      },
      "outputs": [],
      "source": [
        "### Code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b771f95",
      "metadata": {
        "id": "2b771f95"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cc407c",
      "metadata": {
        "id": "78cc407c"
      },
      "source": [
        "**2.2.6 Apply the following pre-processing steps:**\n",
        "\n",
        "1. Convert the label from a Pandas series to a Numpy (m x 1) vector. If you don't do this, it may cause problems when implementing the logistic regression model.\n",
        "2. Split the dataset into training (60%), validation (20%), and test (20%) sets.\n",
        "3. Standardize the columns in the feature matrices. To avoid information leakage, learn the standardization parameters from training, and then apply training, validation and test dataset.\n",
        "4. Add a column of ones to the feature matrices of train, validation and test dataset. This is a common trick so that we can learn a coefficient for the bias term of a linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc12fd6",
      "metadata": {
        "id": "bcc12fd6"
      },
      "outputs": [],
      "source": [
        "### Code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a780961",
      "metadata": {
        "id": "1a780961"
      },
      "source": [
        "### Implement Logisitc Regression\n",
        "\n",
        "We will now implement logistic regression with L2 regularization. Given an (m x n) feature matrix $X$, an (m x 1) label vector $y$, and an (n x 1) weight vector $w$, the hypothesis function for logistic regression is:\n",
        "\n",
        "$$\n",
        "y = \\sigma(X w)\n",
        "$$\n",
        "\n",
        "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, i.e. the sigmoid function. This function scales the prediction to be a probability between 0 and 1, and can then be thresholded to get a discrete class prediction.\n",
        "\n",
        "Just as with linear regression, our objective in logistic regression is to learn the weights $ð‘¤$ which best fit the data. For L2-regularized logistic regression, we find an optimal $w$ to minimize the following loss function:\n",
        "\n",
        "$$\n",
        "\\min_{w} \\ -y^T \\ \\text{log}(\\sigma(Xw)) \\ - \\  (\\mathbf{1} - y)^T \\ \\text{log}(\\mathbf{1} - \\sigma(Xw)) \\ + \\ \\alpha \\| w \\|^2_2 \\\\\n",
        "$$\n",
        "\n",
        "Unlike linear regression, however, logistic regression has no closed-form solution for the optimal $w$. So, we will use gradient descent to find the optimal $w$. The (n x 1) gradient vector $g$ for the loss function above is:\n",
        "\n",
        "$$\n",
        "g = X^T \\Big(\\sigma(Xw) - y\\Big) + 2 \\alpha w\n",
        "$$\n",
        "\n",
        "Below is pseudocode for gradient descent to find the optimal $w$. You should first initialize $w$ (e.g. to a (n x 1) zero vector). Then, for some number of epochs $t$, you should update $w$ with $w - \\eta g $, where $\\eta$ is the learning rate and $g$ is the gradient. You can learn more about gradient descent [here](https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM).\n",
        "\n",
        "> $w = \\mathbf{0}$\n",
        ">\n",
        "> $\\text{for } i = 1, 2, ..., t$\n",
        ">\n",
        "> $\\quad \\quad w = w - \\eta g $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e3aab1c",
      "metadata": {
        "id": "4e3aab1c"
      },
      "source": [
        "**A LogisticRegression class with five methods: train, predict, calculate_loss, calculate_gradient, and calculate_sigmoid has been implemented for you below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4dc1f4",
      "metadata": {
        "id": "5a4dc1f4"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression():\n",
        "    '''\n",
        "    Logistic regression model with L2 regularization.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    alpha: regularization parameter\n",
        "    t: number of epochs to run gradient descent\n",
        "    eta: learning rate for gradient descent\n",
        "    w: (n x 1) weight vector\n",
        "    '''\n",
        "\n",
        "    def __init__(self, alpha=0, t=100, eta=1e-3):\n",
        "        self.alpha = alpha\n",
        "        self.t = t\n",
        "        self.eta = eta\n",
        "        self.w = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        '''Trains logistic regression model using gradient descent\n",
        "        (sets w to its optimal value).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (m x n) feature matrix\n",
        "        y: (m x 1) label vector\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        losses: (t x 1) vector of losses at each epoch of gradient descent\n",
        "        '''\n",
        "\n",
        "        loss = list()\n",
        "        self.w = np.zeros((X.shape[1],1))\n",
        "        for i in range(self.t):\n",
        "            self.w = self.w - (self.eta * self.calculate_gradient(X, y))\n",
        "            loss.append(self.calculate_loss(X, y))\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''Predicts on X using trained model. Make sure to threshold\n",
        "        the predicted probability to return a 0 or 1 prediction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (m x n) feature matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: (m x 1) 0/1 prediction vector\n",
        "        '''\n",
        "        y_pred = self.calculate_sigmoid(X.dot(self.w))\n",
        "        y_pred[y_pred >= 0.5] = 1\n",
        "        y_pred[y_pred < 0.5] = 0\n",
        "        return y_pred\n",
        "\n",
        "    def calculate_loss(self, X, y):\n",
        "        '''Calculates the logistic regression loss using X, y, w,\n",
        "        and alpha. Useful as a helper function for train().\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (m x n) feature matrix\n",
        "        y: (m x 1) label vector\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss: (scalar) logistic regression loss\n",
        "        '''\n",
        "        return -y.T.dot(np.log(self.calculate_sigmoid(X.dot(self.w)))) - (1-y).T.dot(np.log(1-self.calculate_sigmoid(X.dot(self.w)))) + self.alpha*np.linalg.norm(self.w, ord=2)**2\n",
        "\n",
        "    def calculate_gradient(self, X, y):\n",
        "        '''Calculates the gradient of the logistic regression loss\n",
        "        using X, y, w, and alpha. Useful as a helper function\n",
        "        for train().\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (m x n) feature matrix\n",
        "        y: (m x 1) label vector\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: (n x 1) gradient vector for logistic regression loss\n",
        "        '''\n",
        "        return X.T.dot(self.calculate_sigmoid( X.dot(self.w)) - y) + 2*self.alpha*self.w\n",
        "\n",
        "\n",
        "    def calculate_sigmoid(self, x):\n",
        "        '''Calculates the sigmoid function on each element in vector x.\n",
        "        Useful as a helper function for predict(), calculate_loss(),\n",
        "        and calculate_gradient().\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: (m x 1) vector\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sigmoid_x: (m x 1) vector of sigmoid on each element in x\n",
        "        '''\n",
        "        return (1)/(1 + np.exp(-x.astype('float')))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4dfb0d9",
      "metadata": {
        "id": "d4dfb0d9"
      },
      "source": [
        "**2.2.7 Plot Loss over Epoch and Search the space randomly to find best hyperparameters.**\n",
        "\n",
        "i) Using your implementation above, train a logistic regression model **(alpha=0, t=100, eta=1e-3)** on the loan training data. Plot the training loss over epochs. Make sure to label your axes. You should see the loss decreasing and start to converge.\n",
        "\n",
        "ii) Using **alpha between (0,1), eta between(0, 0.001) and t between (0, 100)**, find the best hyperparameters for LogisticRegression. You can randomly search the space 20 times to find the best hyperparameters.\n",
        "\n",
        "iii) Compare accuracy on the test dataset for both the scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da26ae9f",
      "metadata": {
        "id": "da26ae9f"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73caffb",
      "metadata": {
        "id": "d73caffb"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865025f5",
      "metadata": {
        "id": "865025f5"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f3381c8",
      "metadata": {
        "id": "2f3381c8"
      },
      "outputs": [],
      "source": [
        "### Code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f5c6c4",
      "metadata": {
        "id": "47f5c6c4"
      },
      "source": [
        "### Feature Importance\n",
        "\n",
        "**2.2.8 Interpret your trained model using a bar chart of the model weights. Make sure to label the bars (x-axis) and don't forget the bias term!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a560c0",
      "metadata": {
        "id": "f9a560c0"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df811b6",
      "metadata": {
        "id": "8df811b6"
      },
      "outputs": [],
      "source": [
        "### Comment here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28894ca9",
      "metadata": {
        "id": "28894ca9"
      },
      "source": [
        "### 2.3 Support Vector Machines\n",
        "\n",
        "In this part, we will be using support vector machines for classification on the loan dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbef969d",
      "metadata": {
        "id": "cbef969d"
      },
      "source": [
        "### Train Primal SVM\n",
        "**2.3.1 Train a primal SVM (with default parameters) on the loan dataset. Make predictions and report the accuracy on the training, validation, and test sets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e2421f",
      "metadata": {
        "id": "41e2421f"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19b2722",
      "metadata": {
        "id": "c19b2722"
      },
      "source": [
        "### Train Dual SVM\n",
        "**2.3.2 Train a dual SVM (with default parameters) on the heart disease dataset. Make predictions and report the accuracy on the training, validation, and test sets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea2da3d",
      "metadata": {
        "id": "3ea2da3d"
      },
      "outputs": [],
      "source": [
        "### Code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034a55dc",
      "metadata": {
        "id": "034a55dc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "9b79711b",
        "30bb5238",
        "1a780961"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}